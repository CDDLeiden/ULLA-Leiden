{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ./init.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigments on uncertainty quantification and active learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [practical](../03_uncertainty_active_learning.ipynb), we introduced the concepts of uncertainty quantification\n",
    "and active learning. In the examples, we tried to predict calculated logP values which were synthetic data without noise.\n",
    "\n",
    "In this assignment, you will apply tools and concepts from the practical, but on more realistic data, _i.e._ data with noise.\n",
    "\n",
    "*Note: Only the conclusions, data and code presented in this notebook will be considered for grading. If there are other files that support your discussion, make sure they are clearly linked in this notebook.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Uncertainty estimation - effect of noise\n",
    "\n",
    "You can evaluate the effect of noise on the uncertainty estimation by adding synthetic `noise` to the clogP values\n",
    "\n",
    "> data['clogP'] = data['clogP'] + np.random.uniform(-`noise`, `noise`, size=len(data))\n",
    "\n",
    "By experimenting with output labels with a couple of different amplitudes of noise, try to answer how (and why) does the addition of noise affect\n",
    "1. the estimated _aleatoric_ uncertainty,\n",
    "2. the estimated _epistemic_ uncertainty,\n",
    "3. the _quality_ of the uncertainty estimation?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Uncertainty estimation - effect of model complexity\n",
    "\n",
    "The source of the epistemic uncertainty is mainly the lack of data, but the model complexity can affect it too.\n",
    "\n",
    "By experimenting with a couple of different model complexity levels (by adding or removing layers*/hidden_nodes), try to answer how (and why) does the model complexity affect\n",
    "1. the estimated _aleatoric_ uncertainty,\n",
    "2. the estimated _epistemic_ uncertainty,\n",
    "3. the _quality_ of the uncertainty estimation?\n",
    "\n",
    "*You need to modify the layers in `self.linear_relu_stack`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Active learning - selection strategy\n",
    "\n",
    "Now, let's move real experimental data and create a model for monoglyceride lipase (MGLL, [Q99685](https://www.uniprot.org/uniprotkb/Q99685/entry)) bioactivity predictions, and try to improve the model with active learning.\n",
    "\n",
    "In the practical, we used molecular similarity to select the new training compounds at each active learning iteration. However, other selection strategies can be used: aleatoric/epistemic/total uncertainty, (Tanimoto) distance/similarity to the training set, or randomly.\n",
    "\n",
    "Build active learning loops for the MGLL activity predictions with a couple of different selection strategies. Please discuss in advance why you selected these strategies and afterwards discuss their different performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a2bproject_jan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb78dc314653a27287b19b8cfc9f3f4b2c88d6478009f490769a5c0d9a0841cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
